{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22934,"status":"ok","timestamp":1730906666620,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":300},"id":"B1LeH-i9ct-Y","outputId":"11fce761-f10e-4975-c54e-df8858a9dda0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3910,"status":"ok","timestamp":1726016903879,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":240},"id":"hB1jm6GYeP-W","outputId":"af72bcdc-b772-4159-b2b4-06dcd5503772"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n"]}],"source":["! pip install pydub"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3964,"status":"ok","timestamp":1726014621041,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":240},"id":"sOhSHe52fOuv","outputId":"a149f214-42ee-4d39-bd32-ff3a2f1c7854"},"outputs":[{"name":"stdout","output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"]}],"source":["!apt-get install -y ffmpeg"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16943,"status":"ok","timestamp":1726014953579,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":240},"id":"dloFvF5UgZpj","outputId":"85247416-1e7d-4b5a-89f7-7af1d29fe5b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n","\u001b[33m\r0% [Connecting to archive.ubuntu.com] [1 InRelease 12.7 kB/129 kB 10%] [Connect\u001b[0m\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n","Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n","Ign:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n","Get:5 https://r2u.stat.illinois.edu/ubuntu jammy Release [5,713 B]\n","Get:6 https://r2u.stat.illinois.edu/ubuntu jammy Release.gpg [793 B]\n","Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n","Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n","Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [998 kB]\n","Get:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n","Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,224 kB]\n","Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n","Get:13 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,149 kB]\n","Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,293 kB]\n","Hit:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n","Hit:16 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n","Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,572 kB]\n","Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,499 kB]\n","Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,438 kB]\n","Fetched 19.5 MB in 3s (7,288 kB/s)\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","51 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","libopus0 is already the newest version (1.3.1-0.1build2).\n","libopus0 set to manually installed.\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","The following additional packages will be installed:\n","  libpcap0.8\n","The following NEW packages will be installed:\n","  libpcap0.8 opus-tools\n","0 upgraded, 2 newly installed, 0 to remove and 51 not upgraded.\n","Need to get 225 kB of archives.\n","After this operation, 622 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpcap0.8 amd64 1.10.1-4build1 [145 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 opus-tools amd64 0.1.10-1.1 [79.7 kB]\n","Fetched 225 kB in 1s (272 kB/s)\n","debconf: unable to initialize frontend: Dialog\n","debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n","debconf: falling back to frontend: Readline\n","debconf: unable to initialize frontend: Readline\n","debconf: (This frontend requires a controlling tty.)\n","debconf: falling back to frontend: Teletype\n","dpkg-preconfigure: unable to re-open stdin: \n","Selecting previously unselected package libpcap0.8:amd64.\n","(Reading database ... 123597 files and directories currently installed.)\n","Preparing to unpack .../libpcap0.8_1.10.1-4build1_amd64.deb ...\n","Unpacking libpcap0.8:amd64 (1.10.1-4build1) ...\n","Selecting previously unselected package opus-tools.\n","Preparing to unpack .../opus-tools_0.1.10-1.1_amd64.deb ...\n","Unpacking opus-tools (0.1.10-1.1) ...\n","Setting up libpcap0.8:amd64 (1.10.1-4build1) ...\n","Setting up opus-tools (0.1.10-1.1) ...\n","Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n","\n","/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n","\n","Processing triggers for man-db (2.10.2-1) ...\n"]}],"source":["!sudo apt update\n","!sudo apt install ffmpeg libopus0 opus-tools"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":69787,"status":"ok","timestamp":1727276689094,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":240},"id":"d2kLbVX6hF_t","outputId":"9eb49e20-27b2-448a-8421-e9b4e3f83503"},"outputs":[{"name":"stdout","output_type":"stream","text":["Copying wb9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wheels On The Bus/wb8.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus/wb8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wheels On The Bus/wb6.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus/wb6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wheels On The Bus/wheels on bus.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus/wheels on bus.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus\n","Copying wb10.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wheels On The Bus/Wb1.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus/Wb1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus\n","Copying wb3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wheels On The Bus/wb5.opus to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus/wb5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wheels On The Bus/wb4.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus/wb4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wheels On The Bus/wb2.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus/wb2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus\n","Copying wb7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus\n","Copying bs7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Baby Shark/bs2.opus to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark/bs2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Baby Shark/bs3.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark/bs3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Baby Shark/bs4.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark/bs4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Baby Shark/bs8.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark/bs8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark\n","Copying bs5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Baby Shark/bs9.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark/bs9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Baby Shark/bs1.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark/bs1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Baby Shark/bs6.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark/bs6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Baby Shark\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Old Mc Donald/od3.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald/od3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Old Mc Donald/od4.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald/od4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Copying od7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Old Mc Donald/od2.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald/od2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Old Mc Donald/od1.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald/od1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Copying od8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Old Mc Donald/od5.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald/od5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Old Mc Donald/od11.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald/od11.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Copying od9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Old Mc Donald/od10.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald/od10.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Old Mc Donald/od6.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald/od6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Old Mc Donald/od12.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald/od12.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Old Mc Donald\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wish You a Merry Christmas/mc3.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas/mc3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wish You a Merry Christmas/mc1.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas/mc1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wish You a Merry Christmas/mc6.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas/mc6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wish You a Merry Christmas/mc2.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas/mc2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wish You a Merry Christmas/mc4.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas/mc4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wish You a Merry Christmas/mc7.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas/mc7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas\n","Copying mc5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wish You a Merry Christmas/mc8.opus to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas/mc8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas\n","Copying mc10.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wish You a Merry Christmas/mc11.aac to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas/mc11.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas\n","Converted /content/drive/MyDrive/DataSet Of Sounds/Wish You a Merry Christmas/mc9.m4a to MP3\n","Saved /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas/mc9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wish You a Merry Christmas\n","Copying bs7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Baby Shark\n","Copying bs2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Baby Shark\n","Copying bs3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Baby Shark\n","Copying bs4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Baby Shark\n","Copying bs9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Baby Shark\n","Copying bs5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Baby Shark\n","Copying bs1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Baby Shark\n","Copying bs8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Baby Shark\n","Copying bs6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Baby Shark\n","Copying od7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od11.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od12.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od10.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Old Mc Donald\n","Copying wb10.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying Wb1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wheels on bus.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying mc3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc10.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc11.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying bs9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Baby Shark\n","Copying bs7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Baby Shark\n","Copying bs8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Baby Shark\n","Copying bs6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Baby Shark\n","Copying bs5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Baby Shark\n","Copying bs4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Baby Shark\n","Copying bs2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Baby Shark\n","Copying bs3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Baby Shark\n","Copying bs1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Baby Shark\n","Copying od12.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od11.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od10.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying od4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Old Mc Donald\n","Copying wb9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wheels on bus.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying wb10.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying Wb1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wheels On The Bus\n","Copying mc9.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc8.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc7.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc6.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc5.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc4.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc2.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc3.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc11.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc10.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wish You a Merry Christmas\n","Copying mc1.mp3 to /content/drive/MyDrive/DataSet Of Sounds/converted_mp3/converted_mp3/converted_mp3/Wish You a Merry Christmas\n"]}],"source":["import os\n","import subprocess\n","import shutil\n","\n","# Function to convert any audio file to mp3 and save all mp3 files in a new directory, preserving folder structure\n","def convert_all_to_mp3(directory, output_directory):\n","    # Supported audio file extensions except mp3\n","    supported_extensions = ('.aac', '.opus', '.m4a', '.wav', '.flac')\n","\n","    for root, dirs, files in os.walk(directory):\n","        for file_name in files:\n","            input_file = os.path.join(root, file_name)\n","\n","            # Preserve the relative path in the output folder\n","            relative_path = os.path.relpath(root, directory)\n","            output_dir_with_structure = os.path.join(output_directory, relative_path)\n","\n","            # Create the folder structure in the output directory if it doesn't exist\n","            if not os.path.exists(output_dir_with_structure):\n","                os.makedirs(output_dir_with_structure)\n","\n","            # Destination for the converted or copied mp3 file in the new directory\n","            output_file = os.path.join(output_dir_with_structure, file_name.rsplit('.', 1)[0] + '.mp3')\n","\n","            # If the file is already an mp3, copy it to the new directory with folder structure\n","            if file_name.lower().endswith('.mp3'):\n","                print(f\"Copying {file_name} to {output_dir_with_structure}\")\n","                shutil.copy(input_file, output_file)\n","                continue\n","\n","            # Check if the file has a supported non-mp3 extension\n","            if file_name.lower().endswith(supported_extensions):\n","                # Temporary file for conversion\n","                temp_output_file = os.path.join(root, file_name.rsplit('.', 1)[0] + '_temp.mp3')\n","\n","                try:\n","                    # Construct the ffmpeg command to convert any file to .mp3\n","                    command = ['ffmpeg', '-i', input_file, '-codec:a', 'libmp3lame', temp_output_file]\n","                    # Run the command\n","                    subprocess.run(command, check=True)\n","                    print(f\"Converted {input_file} to MP3\")\n","\n","                    # Move the converted mp3 file to the output directory, preserving folder structure\n","                    shutil.move(temp_output_file, output_file)\n","                    print(f\"Saved {output_file} to {output_dir_with_structure}\")\n","\n","                except subprocess.CalledProcessError as e:\n","                    print(f\"Error converting {input_file}: {e}\")\n","\n","# Path to the dataset folder\n","dataset_folder = '/content/drive/MyDrive/DataSet Of Sounds'\n","\n","# Path to the output folder for converted mp3 files, preserving folder structure\n","output_folder = '/content/drive/MyDrive/DataSet Of Sounds/converted_mp3'\n","\n","# Convert all supported audio files in the dataset to mp3 and save to the output folder with structure\n","convert_all_to_mp3(dataset_folder, output_folder)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZNnX4muuhcO"},"outputs":[],"source":["# Imports\n","import os\n","import librosa\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D,Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xTIttMAjtIr"},"outputs":[],"source":["dataset_folder = '/content/drive/MyDrive/DataSet Of Sounds/converted_mp3'\n","\n","\n","# Extract audio features using MFCC, Chroma, and Mel Spectrogram\n","def extract_features(file_name):\n","    audio, sample_rate = librosa.load(file_name, res_type='scipy')  # Load audio file\n","    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=60)  # Increase MFCC coefficients to 60\n","    mfccs_scaled = np.mean(mfccs.T, axis=0)\n","\n","    # Extract additional features\n","    chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n","    chroma_scaled = np.mean(chroma.T, axis=0)\n","\n","    mel = librosa.feature.melspectrogram(y=audio, sr=sample_rate)\n","    mel_scaled = np.mean(mel.T, axis=0)\n","\n","    # Concatenate all features\n","    return np.hstack((mfccs_scaled, chroma_scaled, mel_scaled))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K35Imsg7v2a5"},"outputs":[],"source":["# Prepare dataset\n","def prepare_dataset(dataset_folder):\n","    features = []\n","    labels = []\n","\n","    for folder in os.listdir(dataset_folder):\n","        folder_path = os.path.join(dataset_folder, folder)\n","        if os.path.isdir(folder_path):  # Check if it's a directory\n","            for file in os.listdir(folder_path):\n","                if file.endswith(\".mp3\"):  # Assuming files are in .mp3 format\n","                    file_path = os.path.join(folder_path, file)\n","                    # Extract features and store them with their labels\n","                    features.append(extract_features(file_path))\n","                    labels.append(folder)  # Use the folder name as the label\n","\n","    return np.array(features), np.array(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zLhEVzaI3JWP"},"outputs":[],"source":["# Load the dataset\n","X, y = prepare_dataset(dataset_folder)\n","\n","# Encode labels to numerical values\n","le = LabelEncoder()\n","y_encoded = le.fit_transform(y)\n","\n","# Split data into training, validation, and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":209,"status":"ok","timestamp":1729094621686,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":240},"id":"KkjzpFEj1cae","outputId":"c03eed9d-aa91-4445-c4fb-5e4661689080"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy with SVM: 42.86%\n","Test Accuracy with SVM: 44.44%\n"]}],"source":["# Feature Scaling (Normalization)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_val = scaler.transform(X_val)\n","X_test = scaler.transform(X_test)\n","\n","# Support Vector Machine (SVM) Classifier\n","svm_model = SVC(kernel='linear', random_state=42)  # You can experiment with different kernels like 'rbf'\n","svm_model.fit(X_train, y_train)\n","\n","# Evaluate SVM on validation set\n","y_val_pred_svm = svm_model.predict(X_val)\n","val_accuracy_svm = accuracy_score(y_val, y_val_pred_svm)\n","print(f\"Validation Accuracy with SVM: {val_accuracy_svm * 100:.2f}%\")\n","\n","# Evaluate SVM on test set\n","y_test_pred_svm = svm_model.predict(X_test)\n","test_accuracy_svm = accuracy_score(y_test, y_test_pred_svm)\n","print(f\"Test Accuracy with SVM: {test_accuracy_svm * 100:.2f}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1816,"status":"ok","timestamp":1729094630281,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":240},"id":"4N0JiJhj-k1g","outputId":"12bd85c0-8e22-4ffe-a6ec-09bc96b872b9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy with RandomForest: 42.86%\n","Test Accuracy with RandomForest: 44.44%\n"]}],"source":["# Random Forest Classifier (a strong classifier for structured data)\n","rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n","rf_model.fit(X_train, y_train)\n","\n","# Evaluate on validation set\n","y_val_pred = rf_model.predict(X_val)\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","print(f\"Validation Accuracy with RandomForest: {val_accuracy * 100:.2f}%\")\n","\n","# Evaluate on test set\n","y_test_pred = rf_model.predict(X_test)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","print(f\"Test Accuracy with RandomForest: {test_accuracy * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":826,"status":"ok","timestamp":1729094633993,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":240},"id":"g18_HVyv1ZHI","outputId":"b249b0ab-3716-4017-a26c-326d6fea8d87"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy with RandomForest: 42.86%\n","Test Accuracy with RandomForest: 44.44%\n"]}],"source":["# Random Forest Classifier (a strong classifier for structured data)\n","rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n","rf_model.fit(X_train, y_train)\n","\n","# Evaluate on validation set\n","y_val_pred = rf_model.predict(X_val)\n","val_accuracy = accuracy_score(y_val, y_val_pred)\n","print(f\"Validation Accuracy with RandomForest: {val_accuracy * 100:.2f}%\")\n","\n","# Evaluate on test set\n","y_test_pred = rf_model.predict(X_test)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","print(f\"Test Accuracy with RandomForest: {test_accuracy * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":367,"status":"ok","timestamp":1729094636915,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":240},"id":"on6MlOPU4WyH","outputId":"489a8b59-3d27-443b-d3eb-1e054eb4e9d5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy with MLP: 57.14%\n","Test Accuracy with MLP: 44.44%\n"]}],"source":["#MLP Classifier (Neural Network)\n","mlp_model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=500, random_state=42)\n","mlp_model.fit(X_train, y_train)\n","\n","# Evaluate MLP on validation set\n","y_val_pred_mlp = mlp_model.predict(X_val)\n","val_accuracy_mlp = accuracy_score(y_val, y_val_pred_mlp)\n","print(f\"Validation Accuracy with MLP: {val_accuracy_mlp * 100:.2f}%\")\n","\n","# Evaluate MLP on test set\n","y_test_pred_mlp = mlp_model.predict(X_test)\n","test_accuracy_mlp = accuracy_score(y_test, y_test_pred_mlp)\n","print(f\"Test Accuracy with MLP: {test_accuracy_mlp * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19601,"status":"ok","timestamp":1729094663723,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":240},"id":"g6BBHE2d7QqF","outputId":"65df0cec-93d6-4efb-a101-173d5ee87e75"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n","  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - accuracy: 0.3704 - loss: 1.7706 - val_accuracy: 0.2857 - val_loss: 1.4252\n","Epoch 2/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.4444 - loss: 1.2508 - val_accuracy: 0.4286 - val_loss: 1.4542\n","Epoch 3/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step - accuracy: 0.4815 - loss: 1.2760 - val_accuracy: 0.2857 - val_loss: 1.4894\n","Epoch 4/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step - accuracy: 0.6667 - loss: 1.0278 - val_accuracy: 0.2857 - val_loss: 1.4999\n","Epoch 5/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - accuracy: 0.7037 - loss: 0.7828 - val_accuracy: 0.2857 - val_loss: 1.4828\n","Epoch 6/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - accuracy: 0.7037 - loss: 0.6251 - val_accuracy: 0.1429 - val_loss: 1.4722\n","Epoch 7/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 293ms/step - accuracy: 0.5926 - loss: 0.8318 - val_accuracy: 0.1429 - val_loss: 1.4536\n","Epoch 8/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step - accuracy: 0.7778 - loss: 0.5405 - val_accuracy: 0.1429 - val_loss: 1.4427\n","Epoch 9/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 291ms/step - accuracy: 0.8148 - loss: 0.4332 - val_accuracy: 0.1429 - val_loss: 1.4305\n","Epoch 10/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 319ms/step - accuracy: 0.8519 - loss: 0.5008 - val_accuracy: 0.4286 - val_loss: 1.4209\n","Epoch 11/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 294ms/step - accuracy: 0.8519 - loss: 0.4177 - val_accuracy: 0.4286 - val_loss: 1.4183\n","Epoch 12/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 323ms/step - accuracy: 0.8519 - loss: 0.3385 - val_accuracy: 0.4286 - val_loss: 1.4142\n","Epoch 13/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 264ms/step - accuracy: 0.9259 - loss: 0.2446 - val_accuracy: 0.5714 - val_loss: 1.4085\n","Epoch 14/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 292ms/step - accuracy: 0.8519 - loss: 0.4132 - val_accuracy: 0.5714 - val_loss: 1.4068\n","Epoch 15/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 311ms/step - accuracy: 0.8519 - loss: 0.3120 - val_accuracy: 0.5714 - val_loss: 1.4099\n","Epoch 16/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 300ms/step - accuracy: 0.7037 - loss: 0.5442 - val_accuracy: 0.4286 - val_loss: 1.4149\n","Epoch 17/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - accuracy: 0.8148 - loss: 0.4423 - val_accuracy: 0.4286 - val_loss: 1.4202\n","Epoch 18/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 107ms/step - accuracy: 0.8519 - loss: 0.3679 - val_accuracy: 0.5714 - val_loss: 1.4243\n","Epoch 19/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8889 - loss: 0.2538 - val_accuracy: 0.5714 - val_loss: 1.4338\n","Epoch 20/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step - accuracy: 0.8148 - loss: 0.4008 - val_accuracy: 0.5714 - val_loss: 1.4442\n","Epoch 21/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - accuracy: 0.8519 - loss: 0.2418 - val_accuracy: 0.5714 - val_loss: 1.4539\n","Epoch 22/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 187ms/step - accuracy: 0.9259 - loss: 0.2243 - val_accuracy: 0.5714 - val_loss: 1.4611\n","Epoch 23/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 250ms/step - accuracy: 0.9259 - loss: 0.2096 - val_accuracy: 0.5714 - val_loss: 1.4639\n","Epoch 24/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - accuracy: 0.8519 - loss: 0.3598 - val_accuracy: 0.5714 - val_loss: 1.4647\n","Epoch 25/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - accuracy: 0.8519 - loss: 0.3040 - val_accuracy: 0.5714 - val_loss: 1.4629\n","Epoch 26/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - accuracy: 0.9259 - loss: 0.2355 - val_accuracy: 0.4286 - val_loss: 1.4630\n","Epoch 27/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - accuracy: 0.8148 - loss: 0.3051 - val_accuracy: 0.4286 - val_loss: 1.4633\n","Epoch 28/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 129ms/step - accuracy: 0.9259 - loss: 0.2651 - val_accuracy: 0.4286 - val_loss: 1.4678\n","Epoch 29/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step - accuracy: 0.9259 - loss: 0.1571 - val_accuracy: 0.4286 - val_loss: 1.4703\n","Epoch 30/30\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - accuracy: 0.9259 - loss: 0.1894 - val_accuracy: 0.2857 - val_loss: 1.4773\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.4444 - loss: 1.2025\n","Test Accuracy with CNN: 44.44%\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n","Validation Accuracy with CNN: 28.57%\n"]}],"source":["# Reshape data for 1D CNN\n","X_train_cnn = np.expand_dims(X_train, axis=-1)  # Reshape into 3D array for 1D CNN (samples, time_steps, features)\n","X_val_cnn = np.expand_dims(X_val, axis=-1)\n","X_test_cnn = np.expand_dims(X_test, axis=-1)\n","\n","# One-hot encode the labels for CNN classification\n","y_train_cnn = to_categorical(y_train)\n","y_val_cnn = to_categorical(y_val)\n","y_test_cnn = to_categorical(y_test)\n","\n","# Create 1D CNN model\n","def create_1d_cnn_model(input_shape, num_classes):\n","    model = Sequential()\n","\n","    # 1st Conv Layer (1D)\n","    model.add(Conv1D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Dropout(0.3))\n","\n","    # 2nd Conv Layer (1D)\n","    model.add(Conv1D(64, kernel_size=3, activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Dropout(0.3))\n","\n","    # 3rd Conv Layer (1D)\n","    model.add(Conv1D(128, kernel_size=3, activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Dropout(0.3))\n","\n","    # Flatten the output\n","    model.add(Flatten())\n","\n","    # Fully connected Dense layer\n","    model.add(Dense(128, activation='relu'))\n","    model.add(BatchNormalization())\n","    model.add(Dropout(0.4))\n","\n","    # Output Layer\n","    model.add(Dense(num_classes, activation='softmax'))\n","\n","    return model\n","\n","# Define input shape and number of classes\n","input_shape_cnn = (X_train.shape[1], 1)  # (time_steps, features)\n","num_classes_cnn = y_train_cnn.shape[1]\n","\n","# Build CNN model\n","cnn_model = create_1d_cnn_model(input_shape_cnn, num_classes_cnn)\n","\n","# Compile the model\n","cnn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","cnn_model.fit(X_train_cnn, y_train_cnn, validation_data=(X_val_cnn, y_val_cnn), epochs=30, batch_size=32)\n","\n","# Evaluate CNN on the test set\n","test_loss_cnn, test_accuracy_cnn = cnn_model.evaluate(X_test_cnn, y_test_cnn)\n","print(f\"Test Accuracy with CNN: {test_accuracy_cnn * 100:.2f}%\")\n","\n","# Make predictions on validation set\n","y_val_pred_cnn = cnn_model.predict(X_val_cnn)\n","val_accuracy_cnn = accuracy_score(np.argmax(y_val_cnn, axis=1), np.argmax(y_val_pred_cnn, axis=1))\n","print(f\"Validation Accuracy with CNN: {val_accuracy_cnn * 100:.2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iUHNdos99Y-9"},"outputs":[],"source":["# Function to predict a single audio file\n","def predict_single_audio(file_path, model, scaler, label_encoder):\n","    # Extract features from the new audio file\n","    features = extract_features(file_path)\n","\n","    # Reshape for SVM, RandomForest, and MLP (since they expect 2D inputs)\n","    features = features.reshape(1, -1)\n","\n","    # Scale the features (apply the same scaler used during training)\n","    features_scaled = scaler.transform(features)\n","\n","    # Predict the class using the model\n","    prediction = model.predict(features_scaled)\n","\n","    # Decode the prediction to the original label\n","    predicted_label = label_encoder.inverse_transform(prediction)\n","\n","    return predicted_label[0]\n","\n","# Function to predict a single audio file using CNN\n","def predict_single_audio_cnn(file_path, cnn_model, scaler, label_encoder):\n","    # Extract features from the new audio file\n","    features = extract_features(file_path)\n","\n","    # Scale the features (apply the same scaler used during training)\n","    features_scaled = scaler.transform(features.reshape(1, -1))  # Reshape to 2D for scaling (1, n_features)\n","\n","    # Reshape for CNN (since CNN expects 3D inputs)\n","    features_scaled_cnn = features_scaled.reshape(1, features_scaled.shape[1], 1)\n","\n","    # Predict the class using the CNN model\n","    prediction_cnn = cnn_model.predict(features_scaled_cnn)\n","\n","    # Decode the prediction to the original label\n","    predicted_label_cnn = label_encoder.inverse_transform([np.argmax(prediction_cnn)])\n","\n","    return predicted_label_cnn[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3076,"status":"ok","timestamp":1729094689284,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":240},"id":"c3OMfAOx9e_6","outputId":"d3e386ef-b6ff-4b5b-b6cb-15af119d6196"},"outputs":[{"output_type":"stream","name":"stdout","text":["SVM Prediction: Wheels On The Bus\n","Random Forest Prediction: Wheels On The Bus\n","MLP Prediction: Wheels On The Bus\n","\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 358ms/step\n","CNN Prediction: Wheels On The Bus\n"]}],"source":["# Example usage\n","audio_file_path = \"/content/drive/MyDrive/DataSet Of Sounds/converted_mp3/Wheels On The Bus/wb4.mp3\"\n","\n","# Predict using SVM model\n","svm_prediction = predict_single_audio(audio_file_path, svm_model, scaler, le)\n","print(f\"SVM Prediction: {svm_prediction}\")\n","\n","# Predict using Random Forest model\n","rf_prediction = predict_single_audio(audio_file_path, rf_model, scaler, le)\n","print(f\"Random Forest Prediction: {rf_prediction}\")\n","\n","# Predict using MLP model\n","mlp_prediction = predict_single_audio(audio_file_path, mlp_model, scaler, le)\n","print(f\"MLP Prediction: {mlp_prediction}\")\n","\n","# Predict using CNN model\n","cnn_prediction = predict_single_audio_cnn(audio_file_path, cnn_model, scaler, le)\n","print(f\"CNN Prediction: {cnn_prediction}\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":588,"status":"ok","timestamp":1730906680447,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":300},"id":"KZUmO8zF-fvt","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6da7a68c-f09f-4bcb-bc87-d7e877984916"},"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"]}],"source":["#using kaggle dataset\n","! mkdir ~/.kaggle\n","!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"J7XtqMCS-vCA","executionInfo":{"status":"ok","timestamp":1730906685516,"user_tz":300,"elapsed":204,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}}},"outputs":[],"source":["! chmod 600 ~/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":151350,"status":"ok","timestamp":1730906841043,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":300},"id":"NU-2aT1t-zcy","outputId":"85c4432e-1eab-4d6f-ed99-38f626213595"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset URL: https://www.kaggle.com/datasets/jesusrequena/mlend-hums-and-whistles\n","License(s): unknown\n","Downloading mlend-hums-and-whistles.zip to /content\n","100% 11.1G/11.1G [02:29<00:00, 88.2MB/s]\n","100% 11.1G/11.1G [02:29<00:00, 79.8MB/s]\n"]}],"source":["! kaggle datasets download jesusrequena/mlend-hums-and-whistles"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"RLFcelppF92l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730907066875,"user_tz":300,"elapsed":223467,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}},"outputId":"206e420f-5548-4b76-fafa-2764282eb8a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files extracted successfully!\n"]}],"source":["import zipfile\n","import os\n","\n","# Path to the zip file\n","zip_path = '/content/mlend-hums-and-whistles.zip'  # Adjust the path if needed\n","\n","# Directory where the contents will be extracted\n","extract_to = '/content/ML_dataset'  # You can specify another directory here\n","\n","# Create the directory if it doesn't exist\n","if not os.path.exists(extract_to):\n","    os.makedirs(extract_to)\n","\n","# Opening the zip file\n","with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","    # Extracting all the files to the specified directory\n","    zip_ref.extractall(extract_to)\n","\n","print(\"Files extracted successfully!\")\n"]},{"cell_type":"code","source":["!pip uninstall resampy -y  # Uninstall it first\n","!pip install resampy      # Reinstall it\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3mg4AADMKV9x","executionInfo":{"status":"ok","timestamp":1730907078432,"user_tz":300,"elapsed":5222,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}},"outputId":"dfe68732-8a27-4625-fad7-80c63f6b38db"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping resampy as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mCollecting resampy\n","  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from resampy) (1.26.4)\n","Requirement already satisfied: numba>=0.53 in /usr/local/lib/python3.10/dist-packages (from resampy) (0.60.0)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.53->resampy) (0.43.0)\n","Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: resampy\n","Successfully installed resampy-0.4.3\n"]}]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":22856,"status":"ok","timestamp":1730907105250,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"},"user_tz":300},"id":"KNG7ISHJAkso","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7deea58e-1d6f-4da8-92e8-465aaed45612"},"outputs":[{"output_type":"stream","name":"stdout","text":["0.4.3\n"]}],"source":["import os\n","import librosa\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n","import resampy\n","import pickle\n","print(resampy.__version__)  # This will print the version if it's correctly installed\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"2VSDIzZ0AnO1","executionInfo":{"status":"ok","timestamp":1730907245268,"user_tz":300,"elapsed":240,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}}},"outputs":[],"source":["# Directory of the dataset\n","dataset_folder = '/content/ML_dataset'\n","\n","# Define labels based on your dataset structure\n","labels = {\n","    'MLEndHWD_Frozen_Audio_Files': 0,\n","    'MLEndHWD_Hakuna_Audio_Files': 1,\n","    'MLEndHWD_Mamma_Audio_Files': 2,\n","    'MLEndHWD_Panther_Audio_Files': 3,\n","    'MLEndHWD_Potter_Audio_Files': 4,\n","    'MLEndHWD_Rain_Audio_Files': 5,\n","    'MLEndHWD_Showman_Audio_Files': 6,\n","    'MLEndHWD_StarWars_Audio_Files': 7\n","}\n","\n","# Audio processing parameters\n","sample_rate = 22050  # Standard sample rate for audio\n","n_mfcc = 40          # Number of MFCCs to extract\n","max_pad_len = 862    # Max padding length\n","\n","def extract_features(file_name, sample_rate=22050, n_mfcc=40, max_pad_len=862):\n","    # Load audio file with librosa\n","    audio, _ = librosa.load(file_name, sr=sample_rate, res_type='kaiser_fast')\n","    # Extract MFCCs from the audio\n","    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n","\n","    # Calculate the padding width\n","    # Check if the current MFCC has fewer frames than max_pad_len\n","    if mfccs.shape[1] < max_pad_len:\n","        # Pad width is the difference if it's less than max_pad_len\n","        pad_width = max_pad_len - mfccs.shape[1]\n","        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n","    elif mfccs.shape[1] > max_pad_len:\n","        # If more frames than max_pad_len, truncate the excess\n","        mfccs = mfccs[:, :max_pad_len]\n","\n","    return mfccs\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_srKEFlAy9K","outputId":"60584037-714d-4874-b4c8-50f77885ded0","executionInfo":{"status":"ok","timestamp":1730909375386,"user_tz":300,"elapsed":1994861,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch 0 processed and saved.\n","Batch 1 processed and saved.\n","Batch 2 processed and saved.\n","Batch 3 processed and saved.\n","Batch 4 processed and saved.\n","Batch 5 processed and saved.\n","Batch 6 processed and saved.\n","Batch 7 processed and saved.\n","Batch 8 processed and saved.\n","Batch 9 processed and saved.\n","Batch 10 processed and saved.\n","Batch 11 processed and saved.\n","Batch 12 processed and saved.\n","Batch 13 processed and saved.\n","Batch 14 processed and saved.\n","Batch 15 processed and saved.\n","Batch 16 processed and saved.\n","Batch 17 processed and saved.\n","Batch 18 processed and saved.\n","Batch 19 processed and saved.\n","Batch 20 processed and saved.\n","Batch 21 processed and saved.\n","Batch 22 processed and saved.\n","Batch 23 processed and saved.\n","Batch 24 processed and saved.\n","Batch 25 processed and saved.\n","Batch 26 processed and saved.\n","Batch 27 processed and saved.\n","Batch 28 processed and saved.\n","Batch 29 processed and saved.\n","Batch 30 processed and saved.\n","Batch 31 processed and saved.\n","Batch 32 processed and saved.\n","Batch 33 processed and saved.\n","Batch 34 processed and saved.\n","Batch 35 processed and saved.\n","Batch 36 processed and saved.\n","Batch 37 processed and saved.\n","Batch 38 processed and saved.\n","Batch 39 processed and saved.\n","Batch 40 processed and saved.\n","Batch 41 processed and saved.\n","Batch 42 processed and saved.\n","Batch 43 processed and saved.\n","Batch 44 processed and saved.\n","Batch 45 processed and saved.\n","Batch 46 processed and saved.\n","Batch 47 processed and saved.\n","Batch 48 processed and saved.\n","Batch 49 processed and saved.\n","Batch 50 processed and saved.\n","Batch 51 processed and saved.\n","Batch 52 processed and saved.\n","Batch 53 processed and saved.\n","Batch 54 processed and saved.\n","Batch 55 processed and saved.\n","Batch 56 processed and saved.\n","Batch 57 processed and saved.\n","Batch 58 processed and saved.\n","Batch 59 processed and saved.\n","Batch 60 processed and saved.\n","Batch 61 processed and saved.\n","Batch 62 processed and saved.\n","Batch 63 processed and saved.\n","Batch 64 processed and saved.\n","Batch 65 processed and saved.\n","Batch 66 processed and saved.\n"]}],"source":["import os\n","import numpy as np\n","import pickle\n","\n","# Function to process batches of files and labels\n","def process_batch(batch_files, batch_labels, batch_num):\n","    X_batch, y_batch = [], []\n","\n","    for file_path, label in zip(batch_files, batch_labels):\n","        features = extract_features(file_path)  # Assuming extract_features is defined elsewhere\n","        X_batch.append(features)\n","        y_batch.append(label)\n","\n","    X_batch = np.array(X_batch)\n","    y_batch = np.array(y_batch)\n","\n","    # Save batches to disk\n","    with open(f'X_batch_{batch_num}.pkl', 'wb') as f:\n","        pickle.dump(X_batch, f)\n","    with open(f'y_batch_{batch_num}.pkl', 'wb') as f:\n","        pickle.dump(y_batch, f)\n","\n","    print(f'Batch {batch_num} processed and saved.')\n","\n","# Assuming you have a dictionary of folder labels\n","labels = {'MLEndHWD_Frozen_Audio_Files': 0, 'MLEndHWD_Hakuna_Audio_Files': 1, 'MLEndHWD_Mamma_Audio_Files': 2,\n","          'MLEndHWD_Panther_Audio_Files': 3, 'MLEndHWD_Potter_Audio_Files': 4,\n","          'MLEndHWD_Rain_Audio_Files': 5, 'MLEndHWD_Showman_Audio_Files':6, 'MLEndHWD_StarWars_Audio_Files':7}  # Example labels\n","\n","dataset_folder = \"/content/ML_dataset\"  # Ensure this is correct\n","\n","all_files, all_labels = [], []\n","\n","# Loop through each folder and label, collecting files\n","for folder, label in labels.items():\n","    folder_path = os.path.join(dataset_folder, folder)\n","\n","    try:\n","        # Check if folder exists\n","        if not os.path.exists(folder_path):\n","            raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n","\n","        # Get files in the folder\n","        files = [os.path.join(folder_path, f) for f in os.listdir(folder_path)]\n","        all_files.extend(files)\n","        all_labels.extend([label] * len(files))\n","\n","    except FileNotFoundError as e:\n","        print(e)  # Log the error message but continue execution\n","\n","# Batch size and batch processing\n","batch_size = 100\n","for batch_num in range(0, len(all_files), batch_size):\n","    batch_files = all_files[batch_num:batch_num + batch_size]\n","    batch_labels = all_labels[batch_num:batch_num + batch_size]\n","    process_batch(batch_files, batch_labels, batch_num // batch_size)\n"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"OBqweqXoGAnV","executionInfo":{"status":"ok","timestamp":1730909900254,"user_tz":300,"elapsed":151,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}}},"outputs":[],"source":["import pickle\n","import numpy as np\n","\n","def load_batch(batch_num):\n","    # Load the X (features) and y (labels) batches from disk\n","    with open(f'X_batch_{batch_num}.pkl', 'rb') as f:\n","        X_batch = pickle.load(f)\n","    with open(f'y_batch_{batch_num}.pkl', 'rb') as f:\n","        y_batch = pickle.load(f)\n","    return np.array(X_batch), np.array(y_batch)\n"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"v8b4Voe2BGix","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730910562518,"user_tz":300,"elapsed":145,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}},"outputId":"24d3a071-8358-431b-ce63-998ac3dc7879"},"outputs":[{"output_type":"stream","name":"stdout","text":["Labels used for fitting: [0 1 2 3 4 5 6 7]\n"]}],"source":["from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from tensorflow.keras.utils import to_categorical\n","import pickle\n","\n","# Fit LabelEncoder before processing\n","label_encoder = LabelEncoder()\n","label_encoder.fit(list(labels.values()))\n","num_classes = len(labels)\n","\n","# Preprocess batches with the initialized label encoder and scaler\n","scaler = StandardScaler()\n","\n","def preprocess_batch(X_batch, y_batch):\n","    n_samples, n_features, _ = X_batch.shape\n","    X_batch_reshaped = X_batch.reshape((n_samples, n_features * max_pad_len))\n","    X_scaled = scaler.fit_transform(X_batch_reshaped)  # Standardize features\n","    X_scaled = X_scaled.reshape((n_samples, n_features, max_pad_len))\n","\n","    print(f\"Processing labels in batch: {np.unique(y_batch)}\")\n","\n","    try:\n","        y_encoded = label_encoder.transform(y_batch)\n","    except ValueError as e:\n","        print(f\"Error encoding labels: {e}\")\n","        print(f\"All possible labels: {label_encoder.classes_}\")\n","        raise e\n","\n","    y_categorical = to_categorical(y_encoded, num_classes=num_classes)\n","    return X_scaled, y_categorical\n","\n","print(f\"Labels used for fitting: {label_encoder.classes_}\")\n"]},{"cell_type":"code","source":["# Print all unique labels that LabelEncoder was fit on\n","print(f\"Labels used for fitting: {label_encoder.classes_}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8JKtk2jkqwji","executionInfo":{"status":"ok","timestamp":1730910567099,"user_tz":300,"elapsed":150,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}},"outputId":"d9633498-4283-46cf-a621-be6cac04abb6"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Labels used for fitting: [0 1 2 3 4 5 6 7]\n"]}]},{"cell_type":"code","source":["import os\n","import librosa\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n","from tensorflow.keras import Input\n","\n","# Define paths and parameters\n","dataset_folder = '/content/ML_dataset'\n","sample_rate = 22050\n","n_mfcc = 40\n","max_pad_len = 862\n","batch_size = 100\n","epochs = 10\n","\n","# Gather all files and labels using folder names\n","all_files, all_labels = [], []\n","for folder in os.listdir(dataset_folder):\n","    folder_path = os.path.join(dataset_folder, folder)\n","    if os.path.isdir(folder_path):\n","        files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n","        all_files.extend(files)\n","        all_labels.extend([folder] * len(files))\n","\n","# Step 1: Fit LabelEncoder on the original folder names\n","label_encoder = LabelEncoder()\n","label_encoder.fit(all_labels)\n","num_classes = len(label_encoder.classes_)\n","\n","# Define the model with Input layer\n","input_shape = (n_mfcc, max_pad_len)\n","model = Sequential([\n","    Input(shape=input_shape),\n","    Conv1D(64, kernel_size=3, activation='relu'),\n","    BatchNormalization(),\n","    MaxPooling1D(pool_size=2),\n","    Dropout(0.3),\n","    Conv1D(128, kernel_size=3, activation='relu'),\n","    BatchNormalization(),\n","    MaxPooling1D(pool_size=2),\n","    Dropout(0.3),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.3),\n","    Dense(num_classes, activation='softmax')\n","])\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Function to preprocess a batch\n","scaler = StandardScaler()\n"],"metadata":{"id":"mMUH8wVJq8xp","executionInfo":{"status":"ok","timestamp":1730910815017,"user_tz":300,"elapsed":159,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","execution_count":21,"metadata":{"id":"DlP7ZVBFFf01","executionInfo":{"status":"ok","timestamp":1730910842361,"user_tz":300,"elapsed":305,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}}},"outputs":[],"source":["def process_batch(batch_files, batch_labels, batch_num):\n","    X_batch, y_batch = [], []\n","\n","    for file_path, label in zip(batch_files, batch_labels):\n","        features = extract_features(file_path)\n","        X_batch.append(features)\n","        y_batch.append(label)  # Store the string label here, not an integer\n","\n","    X_batch = np.array(X_batch)\n","    y_batch = np.array(y_batch)\n","\n","    # Save batches to disk\n","    with open(f'X_batch_{batch_num}.pkl', 'wb') as f:\n","        pickle.dump(X_batch, f)\n","    with open(f'y_batch_{batch_num}.pkl', 'wb') as f:\n","        pickle.dump(y_batch, f)\n","\n","    print(f'Batch {batch_num} processed and saved.')\n"]},{"cell_type":"code","source":["def preprocess_batch(X_batch, y_batch):\n","    # Check that y_batch has the correct format\n","    print(f\"Batch labels before encoding: {y_batch[:5]}\")  # Debugging line to confirm labels\n","\n","    n_samples, n_features, _ = X_batch.shape\n","    X_batch_reshaped = X_batch.reshape((n_samples, n_features * max_pad_len))\n","    X_scaled = scaler.fit_transform(X_batch_reshaped)\n","    X_scaled = X_scaled.reshape((n_samples, n_features, max_pad_len))\n","\n","    # Encode labels and one-hot encode them\n","    y_encoded = label_encoder.transform(y_batch)  # Convert to integers based on strings\n","    y_categorical = to_categorical(y_encoded, num_classes=num_classes)\n","\n","    return X_scaled, y_categorical\n"],"metadata":{"id":"HjOvxeZqpqa-","executionInfo":{"status":"ok","timestamp":1730910864715,"user_tz":300,"elapsed":135,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["import os\n","import librosa\n","import numpy as np\n","from sklearn.preprocessing import LabelEncoder, StandardScaler\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","from tensorflow.keras import Input\n","from sklearn.model_selection import train_test_split\n","import pickle\n","\n","# Define paths and parameters\n","dataset_folder = '/content/ML_dataset'\n","sample_rate = 22050\n","n_mfcc = 40\n","max_pad_len = 862\n","batch_size = 100\n","epochs = 10\n","\n","# Collect all files and labels using folder names\n","all_files, all_labels = [], []\n","for folder in os.listdir(dataset_folder):\n","    folder_path = os.path.join(dataset_folder, folder)\n","    if os.path.isdir(folder_path):\n","        files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".wav\")]\n","        all_files.extend(files)\n","        all_labels.extend([folder] * len(files))\n","\n","# Step 1: Fit LabelEncoder on original folder names\n","label_encoder = LabelEncoder()\n","label_encoder.fit(all_labels)\n","num_classes = len(label_encoder.classes_)\n","\n","# Split data into training and validation sets\n","train_files, val_files, train_labels, val_labels = train_test_split(\n","    all_files, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n",")\n","\n","# Define the model\n","input_shape = (n_mfcc, max_pad_len)\n","model = Sequential([\n","    Input(shape=input_shape),\n","    Conv1D(64, kernel_size=3, activation='relu'),\n","    BatchNormalization(),\n","    MaxPooling1D(pool_size=2),\n","    Dropout(0.3),\n","    Conv1D(128, kernel_size=3, activation='relu'),\n","    BatchNormalization(),\n","    MaxPooling1D(pool_size=2),\n","    Dropout(0.3),\n","    Flatten(),\n","    Dense(128, activation='relu'),\n","    Dropout(0.3),\n","    Dense(num_classes, activation='softmax')\n","])\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","\n","# Callbacks for early stopping and model checkpointing\n","callbacks = [\n","    EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n","    ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True)  # Use .keras extension\n","]\n","\n","\n","# Function to extract MFCC features and preprocess the data\n","def extract_features(file_name, sample_rate=22050, n_mfcc=40, max_pad_len=862):\n","    audio, _ = librosa.load(file_name, sr=sample_rate, res_type='kaiser_fast')\n","    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)\n","    if mfccs.shape[1] < max_pad_len:\n","        pad_width = max_pad_len - mfccs.shape[1]\n","        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n","    else:\n","        mfccs = mfccs[:, :max_pad_len]\n","    return mfccs\n","\n","# Preprocess batch function\n","scaler = StandardScaler()\n","\n","def preprocess_batch(file_paths, labels):\n","    X_batch = np.array([extract_features(file) for file in file_paths])\n","    n_samples, n_features, _ = X_batch.shape\n","    X_batch_reshaped = X_batch.reshape((n_samples, n_features * max_pad_len))\n","    X_scaled = scaler.fit_transform(X_batch_reshaped)\n","    X_scaled = X_scaled.reshape((n_samples, n_features, max_pad_len))\n","\n","    y_encoded = label_encoder.transform(labels)\n","    y_categorical = to_categorical(y_encoded, num_classes=num_classes)\n","\n","    return X_scaled, y_categorical\n","\n","# Prepare the validation data\n","X_val, y_val = preprocess_batch(val_files, val_labels)\n","\n","# Training loop\n","for epoch in range(epochs):\n","    print(f'Epoch {epoch + 1}/{epochs}')\n","    for batch_num in range(0, len(train_files), batch_size):\n","        batch_files = train_files[batch_num:batch_num + batch_size]\n","        batch_labels = train_labels[batch_num:batch_num + batch_size]\n","\n","        X_batch, y_batch = preprocess_batch(batch_files, batch_labels)\n","        model.train_on_batch(X_batch, y_batch)\n","        print(f'Processed batch {batch_num // batch_size + 1} for epoch {epoch + 1}')\n","\n","    # Evaluate on validation set after each epoch\n","    val_loss, val_accuracy = model.evaluate(X_val, y_val, verbose=0)\n","    print(f'Validation loss: {val_loss}, Validation accuracy: {val_accuracy}')\n","\n","    # Use callbacks to handle early stopping and checkpointing\n","    for callback in callbacks:\n","        callback.on_epoch_end(epoch, logs={'val_loss': val_loss})\n","\n","# Save the scaler and label encoder for future use\n","with open('scaler.pkl', 'wb') as f:\n","    pickle.dump(scaler, f)\n","with open('label_encoder.pkl', 'wb') as f:\n","    pickle.dump(label_encoder, f)\n","\n","print(\"Training complete and model saved.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"krWUsQOWVAJf","executionInfo":{"status":"error","timestamp":1730915421196,"user_tz":300,"elapsed":2138173,"user":{"displayName":"Gowthami Polumuri","userId":"11577614548915949147"}},"outputId":"0046581e-5141-46bf-e6bc-30667dcb1dc3"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","Processed batch 1 for epoch 1\n","Processed batch 2 for epoch 1\n","Processed batch 3 for epoch 1\n","Processed batch 4 for epoch 1\n","Processed batch 5 for epoch 1\n","Processed batch 6 for epoch 1\n","Processed batch 7 for epoch 1\n","Processed batch 8 for epoch 1\n","Processed batch 9 for epoch 1\n","Processed batch 10 for epoch 1\n","Processed batch 11 for epoch 1\n","Processed batch 12 for epoch 1\n","Processed batch 13 for epoch 1\n","Processed batch 14 for epoch 1\n","Processed batch 15 for epoch 1\n","Processed batch 16 for epoch 1\n","Processed batch 17 for epoch 1\n","Processed batch 18 for epoch 1\n","Processed batch 19 for epoch 1\n","Processed batch 20 for epoch 1\n","Processed batch 21 for epoch 1\n","Processed batch 22 for epoch 1\n","Processed batch 23 for epoch 1\n","Processed batch 24 for epoch 1\n","Processed batch 25 for epoch 1\n","Processed batch 26 for epoch 1\n","Processed batch 27 for epoch 1\n","Processed batch 28 for epoch 1\n","Processed batch 29 for epoch 1\n","Processed batch 30 for epoch 1\n","Processed batch 31 for epoch 1\n","Processed batch 32 for epoch 1\n","Processed batch 33 for epoch 1\n","Processed batch 34 for epoch 1\n","Processed batch 35 for epoch 1\n","Processed batch 36 for epoch 1\n","Processed batch 37 for epoch 1\n","Processed batch 38 for epoch 1\n","Processed batch 39 for epoch 1\n","Processed batch 40 for epoch 1\n","Processed batch 41 for epoch 1\n","Processed batch 42 for epoch 1\n","Processed batch 43 for epoch 1\n","Processed batch 44 for epoch 1\n","Processed batch 45 for epoch 1\n","Processed batch 46 for epoch 1\n","Processed batch 47 for epoch 1\n","Processed batch 48 for epoch 1\n","Processed batch 49 for epoch 1\n","Processed batch 50 for epoch 1\n","Processed batch 51 for epoch 1\n","Processed batch 52 for epoch 1\n","Processed batch 53 for epoch 1\n","Validation loss: 1.984408974647522, Validation accuracy: 0.22448979318141937\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'NoneType' object has no attribute 'get_weights'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-710ce51b526d>\u001b[0m in \u001b[0;36m<cell line: 97>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;31m# Use callbacks to handle early stopping and checkpointing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;31m# Save the scaler and label encoder for future use\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/early_stopping.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;31m# If best weights were never set,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m             \u001b[0;31m# then the current weights are the best.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_weights'"]}]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMFzrAUdANDxE0qsg+qoeLR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}